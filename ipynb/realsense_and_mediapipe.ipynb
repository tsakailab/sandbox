{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sNuBXVKBTSM"
   },
   "source": [
    "# ポイントクラウドを用いた平面の検出\n",
    "\n",
    "\n",
    "Q21: ベクトル $\\bf a$ のノルム（長さ）を $\\|\\bf a\\|$と記す．また，ベクトル $\\bf a$ と $\\bf b$の内積を ${\\bf a}\\cdot{\\bf b}$，外積を ${\\bf a}\\times{\\bf b}$ と記す．__これらの記法を用いて__，「位置ベクトル ${\\bf p}_0$，${\\bf p}_1$，${\\bf p}_2$ の3点を通る平面」と「位置ベクトル $\\bf p$ の点」の間の距離 $d$ を求める公式を作れ．\n",
    "\n",
    "Q22: Q21の公式を図を用いて解説せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW-_iTAcHZrP"
   },
   "source": [
    "## RealSenseで平面を撮像してください．\n",
    "> 机や壁と多面体など，なるべく2つ以上の面が広く写るように撮影しましょう．\n",
    "\n",
    "> このファイルの内容は，★印まで pointcloud_rs.ipynb と同じです．★印まで同様に作業を進めてください． "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gwSngoUHZrQ"
   },
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure color and depth to run at VGA resolution at 30 frames per second\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xZKJD5IHZrR"
   },
   "source": [
    "## カラー画像と深度画像を取得して表示・保存します．\n",
    "スペースキーを押す毎に画像が連番で保存されます．'q'を押すと終了します．最後に保存したデータが点群の作成に使われます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Start streaming\n",
    "pipeline = rs.pipeline()\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Get camera parameters\n",
    "intr = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
    "scale = config.resolve(rs.pipeline_wrapper(pipeline)).get_device().first_depth_sensor().get_depth_scale()\n",
    "\n",
    "print(\"focal length(x) in pixels = \", intr.fx)\n",
    "print(\"focal length(y) in pixels = \", intr.fy)\n",
    "print(\"image height = \", intr.height)\n",
    "print(\"image width = \", intr.width)\n",
    "print(\"ppx = \", intr.ppx)\n",
    "print(\"ppy = \", intr.ppy)\n",
    "\n",
    "# Create a camera alignment object (depth aligned to color)\n",
    "align = rs.align(rs.stream.color)\n",
    "max_depth = 2.0 / scale # Zeros out for any depth greater than 2.0 meters\n",
    "\n",
    "# Display and save images\n",
    "print(\"Press [SPACE] to save images (png) and depth data (npy).\")\n",
    "print(\"Press 'q' to stop.\")\n",
    "nsaved = 0\n",
    "try:\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as pose:\n",
    "        while True:\n",
    "            # Wait for a coherent pair of frames: depth and color\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            aligned_frames = align.process(frames)\n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "            depth_frame = aligned_frames.get_depth_frame()\n",
    "            if not depth_frame or not color_frame:\n",
    "                continue\n",
    "\n",
    "            # Convert images to numpy arrays\n",
    "            bgr = np.asanyarray(color_frame.get_data())\n",
    "            depth = np.asanyarray(depth_frame.get_data())\n",
    "            depth[depth > max_depth] = 0 # Zeros out\n",
    "\n",
    "            # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "            depth_colormap = cv2.applyColorMap(cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U), \n",
    "                                               cv2.COLORMAP_JET)\n",
    "\n",
    "            images = np.hstack((bgr, depth_colormap))\n",
    "            images.flags.writeable = False\n",
    "            images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "#             results = pose.process(images) # mediapipe\n",
    "            results_bgr = pose.process(bgr) # mediapipe\n",
    "            results_depth = pose.process(depth_colormap) # mediapipe\n",
    "            # Show images\n",
    "            images.flags.writeable = True\n",
    "            images = cv2.cvtColor(images, cv2.COLOR_RGB2BGR)\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 images,\n",
    "#                 results.pose_landmarks,\n",
    "#                 mp_pose.POSE_CONNECTIONS,\n",
    "#                 landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                bgr,\n",
    "                results_bgr.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                depth_colormap,\n",
    "                results_depth.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "            )\n",
    "#             cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "#             cv2.imshow('RealSense', images)\n",
    "            cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.namedWindow('RealSense2', cv2.WINDOW_AUTOSIZE)\n",
    "            cv2.imshow('RealSense', bgr)\n",
    "            cv2.imshow('RealSense2', depth_colormap)\n",
    "            key = cv2.waitKey(33)\n",
    "            if key == ord(' '):\n",
    "                Z = depth * scale * 1e+3 # unit in mm\n",
    "                color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Save images\n",
    "                cv2.imwrite('color{:02d}pc.png'.format(qqqqqq), bgr)\n",
    "                cv2.imwrite('depth{:02d}pc.png'.format(nsaved), depth_colormap)\n",
    "                np.save('Z{:02d}pc.npy'.format(nsaved), Z)\n",
    "\n",
    "                print(\"color image and depth data are saved ({:02d})\".format(nsaved))\n",
    "                nsaved += 1\n",
    "\n",
    "            elif key == ord('q'):\n",
    "                if nsaved == 0:\n",
    "                    Z = depth * scale * 1e+3 # unit in mm\n",
    "                    color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wv-ooT7HZrS"
   },
   "source": [
    "## 逆透視変換でポイントクラウドを作りましょう．\n",
    "カラー画像および点群を作成する深度データを可視化します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZiWvW_rHZrT"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "height, width, _ = color.shape\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,2,1)\n",
    "# plt.imshow(color)\n",
    "plt.imshow(color)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Z, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pc_plane_detection_rs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528c694a",
   "metadata": {},
   "source": [
    "## MediaPipe Simple Usage\n",
    "https://developers.google.com/mediapipe/solutions/guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e4f75",
   "metadata": {},
   "source": [
    "On Ubuntu, `ls /dev/video*` displays available camera devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfdce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 2 4 6\n",
    "\n",
    "#cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d4bdc",
   "metadata": {},
   "source": [
    "### Hand Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31033c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands\n",
    "hands_mesh = hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "while True:\n",
    "    success, bgr = cap.read()\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for i in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(bgr, i, hands.HAND_CONNECTIONS, \n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color = (255, 0,0),circle_radius=4, thickness=3),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(thickness=3, color=(0,0,255)))\n",
    "\n",
    "    cv2.imshow(\"Hand Landmarks\", bgr)\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5dd3dc",
   "metadata": {},
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2852c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.7)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "    start = time.time()\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find faces\n",
    "    results = face_detection.process(rgb)\n",
    "\n",
    "    # If detected, put\n",
    "    if results.detections:\n",
    "        for id, detection in enumerate(results.detections):\n",
    "\n",
    "            mp_drawing.draw_detection(bgr, detection)\n",
    "            #print(id, detection)\n",
    "\n",
    "            bBox = detection.location_data.relative_bounding_box\n",
    "            h, w, c = bgr.shape\n",
    "            boundBox = int(bBox.xmin * w), int(bBox.ymin * h), int(bBox.width * w), int(bBox.height * h)\n",
    "\n",
    "            cv2.putText(bgr, f'{int(detection.score[0]*100)}%', (boundBox[0], boundBox[1] - 20), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2)\n",
    "\n",
    "    end = time.time()\n",
    "    totalTime = end - start\n",
    "    fps = 1 / totalTime\n",
    "    cv2.putText(bgr, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "    cv2.imshow('Face Detection', bgr)\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd3056",
   "metadata": {},
   "source": [
    "### Pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d3b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb)\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        bgr,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Pose', cv2.flip(bgr, 1))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c8a64",
   "metadata": {},
   "source": [
    "### Holistic landmark detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f379e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(rgb)\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        bgr,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    \n",
    "    mp_drawing.draw_landmarks(\n",
    "        bgr,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    \n",
    "    cv2.imshow('MediaPipe Holistic', cv2.flip(bgr, 1))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21925a96",
   "metadata": {},
   "source": [
    "### object detection\n",
    "\n",
    "mediapipe introduction: https://developers.google.com/mediapipe/solutions/vision/object_detector/index\n",
    "\n",
    "https://developers.google.com/mediapipe/solutions/vision/object_detector/python\n",
    "\n",
    "plz download this detection model and put it as the same folder with this code:\n",
    "https://storage.googleapis.com/mediapipe-tasks/object_detector/efficientdet_lite2_uint8.tflite\n",
    "\n",
    "`\n",
    "!wget -q -O efficientdet_lite2_uint8.tflite -q https://storage.googleapis.com/mediapipe-tasks/object_detector/efficientdet_lite2_uint8.tflite\n",
    "`\n",
    "\n",
    "mediapipe image class:\n",
    "- https://developers.google.com/mediapipe/api/solutions/python/mp/Image\n",
    "- https://developers.google.com/mediapipe/api/solutions/python/mp/ImageFormat\n",
    "\n",
    "mediapipe sample code: https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/object_detection/python/object_detector.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe607039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import ImageFormat\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='efficientdet_lite2_uint8.tflite')\n",
    "options = vision.ObjectDetectorOptions(base_options=base_options,\n",
    "                                       score_threshold=0.5)\n",
    "detector = vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb_mp = mp.Image(image_format=ImageFormat.SRGB, data=rgb)\n",
    "    \n",
    "    detection_result = detector.detect(rgb_mp)\n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "        # Draw bounding_box\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        cv2.rectangle(bgr, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "        # Draw label and score\n",
    "        category = detection.categories[0]\n",
    "        category_name = category.category_name\n",
    "        probability = round(category.score, 2)\n",
    "        result_text = category_name + ' (' + str(probability) + ')'\n",
    "        text_location = (MARGIN + bbox.origin_x,\n",
    "                         MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "        cv2.putText(bgr, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                    FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "    cv2.imshow('MediaPipe Object detection', bgr)\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab0899",
   "metadata": {},
   "source": [
    "### image segmentation (foreground detection)\n",
    "\n",
    "mediapipe introduction: https://developers.google.com/mediapipe/solutions/vision/image_segmenter\n",
    "\n",
    "plz download this detection model and put it as the same folder with this code: https://storage.googleapis.com/mediapipe-assets/deeplabv3.tflite\n",
    "\n",
    "`\n",
    "!wget -O deeplabv3.tflite -q https://storage.googleapis.com/mediapipe-assets/deeplabv3.tflite\n",
    "`\n",
    "\n",
    "mediapipe ImageSegmenterOptions:https://developers.google.com/mediapipe/api/solutions/python/mp/tasks/vision/ImageSegmenterOptions\n",
    "\n",
    "mediapipe sample code: https://github.com/googlesamples/mediapipe/blob/main/examples/image_segmentation/python/image_segmentation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28462453",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='deeplabv3.tflite')\n",
    "options = vision.ImageSegmenterOptions(base_options=base_options,\n",
    "                                      output_type = mp.tasks.vision.ImageSegmenterOptions.OutputType.CATEGORY_MASK\n",
    "                                      )\n",
    "# Create the image segmenter\n",
    "segmenter = vision.ImageSegmenter.create_from_options(options)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb_mp = mp.Image(image_format=ImageFormat.SRGB, data=rgb)\n",
    "    \n",
    "    segmentation_result = segmenter.segment(rgb_mp)\n",
    "    category_mask = segmentation_result[0]\n",
    "    \n",
    "    blurred_image = cv2.GaussianBlur(bgr, (55,55), 0)\n",
    "    condition = np.stack((category_mask.numpy_view(),) * 3, axis=-1) != 0 # foreground(category_mask == 0): False, background: True \n",
    "    output_image = np.where(condition, bgr, blurred_image)\n",
    "\n",
    "    cv2.imshow('MediaPipe Object detection', output_image)\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef47681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "528c694a",
   "metadata": {},
   "source": [
    "# MediaPipe Simple Usage\n",
    "https://developers.google.com/mediapipe/solutions/guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce5e4f75",
   "metadata": {},
   "source": [
    "On Ubuntu, `ls /dev/video*` displays available camera devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbe6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /dev/video*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52dfc23b",
   "metadata": {},
   "source": [
    "## カメラを設定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfdce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# 2 4 6\n",
    "# cap.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644d4bdc",
   "metadata": {},
   "source": [
    "## Hand Landmarks\n",
    "![](https://developers.google.com/static/mediapipe/images/solutions/examples/hand_landmark.png)\n",
    "### References\n",
    "mediapipe introduction:\n",
    "- https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\n",
    "- https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python\n",
    "\n",
    "mediapipe sample code:\n",
    "- https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O hand_detector.task https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3a9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def visualize(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    if len(hand_landmarks_list):\n",
    "        for idx in range(len(hand_landmarks_list)):\n",
    "            hand_landmarks = hand_landmarks_list[idx]\n",
    "            handedness = handedness_list[idx]\n",
    "\n",
    "            # Draw the hand landmarks.\n",
    "            hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            hand_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "            ])\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks_proto,\n",
    "                solutions.hands.HAND_CONNECTIONS,\n",
    "                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "            # Get the top left corner of the detected hand's bounding box.\n",
    "            height, width, _ = annotated_image.shape\n",
    "            x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "            y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "            text_x = int(min(x_coordinates) * width)\n",
    "            text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "            # Draw handedness (left or right hand) on the image.\n",
    "            cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                        (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9046f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='hand_detector.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "while True:\n",
    "    success, bgr = cap.read()\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = cv2.flip(rgb, 1)\n",
    "    mp_image = mp.Image(mp.ImageFormat.SRGB, rgb)\n",
    "    \n",
    "    results = detector.detect(mp_image)\n",
    "    \n",
    "    annotated_image = visualize(mp_image.numpy_view(), results)\n",
    "    cv2.imshow(\"Hand Landmarks\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e5dd3dc",
   "metadata": {},
   "source": [
    "## Face Detection\n",
    "![](https://developers.google.com/static/mediapipe/images/solutions/examples/face_detector.png)\n",
    "### References\n",
    "mediapipe introduction:\n",
    "- https://developers.google.com/mediapipe/solutions/vision/face_detector\n",
    "- https://developers.google.com/mediapipe/solutions/vision/face_detector/python\n",
    "\n",
    "mediapipe sample code:\n",
    "- https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_detector/python/face_detector.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898883e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O face_detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c815bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "\n",
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "\n",
    "    # Checks if the float value is between 0 and 1.\n",
    "    def is_valid_normalized_value(value: float) -> bool:\n",
    "        return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "    if not (is_valid_normalized_value(normalized_x) and is_valid_normalized_value(normalized_y)):\n",
    "        return None\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image,\n",
    "    detection_result\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "    Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "    Returns:\n",
    "    Image with bounding boxes.\n",
    "    \"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    if len(detection_result.detections):\n",
    "        for detection in detection_result.detections:\n",
    "            # Draw bounding_box\n",
    "            bbox = detection.bounding_box\n",
    "            start_point = bbox.origin_x, bbox.origin_y\n",
    "            end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "            cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "        # Draw keypoints\n",
    "        for keypoint in detection.keypoints:\n",
    "            keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,width, height)\n",
    "            color, thickness, radius = (0, 255, 0), 2, 2\n",
    "            cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "        # Draw label and score\n",
    "        category = detection.categories[0]\n",
    "        category_name = category.category_name\n",
    "        category_name = '' if category_name is None else category_name\n",
    "        probability = round(category.score, 2)\n",
    "        result_text = category_name + ' (' + str(probability) + ')'\n",
    "        text_location = (MARGIN + bbox.origin_x,\n",
    "                        MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "        cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                    FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724e3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceDetector object.\n",
    "base_options = python.BaseOptions(model_asset_path='face_detector.tflite')\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = cv2.flip(rgb, 1)\n",
    "    mp_image = mp.Image(mp.ImageFormat.SRGB, rgb)\n",
    "\n",
    "    results = detector.detect(mp_image)\n",
    "    \n",
    "    annotated_image = visualize(rgb, results)\n",
    "    cv2.imshow(\"Hand Landmarks\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f531042d",
   "metadata": {},
   "source": [
    "## Face Mesh\n",
    "![](https://developers.google.com/static/mediapipe/images/solutions/examples/face_landmark.png)\n",
    "### References\n",
    "mediapipe introduction:\n",
    "- https://developers.google.com/mediapipe/solutions/vision/face_landmarker\n",
    "- https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python\n",
    "\n",
    "mediapipe sample code:\n",
    "- https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb#scrollTo=_JVO3rvPD4RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cb11314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n"
     ]
    }
   ],
   "source": [
    "!wget -O facemesh_detector.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebfd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize(rgb_image, detection_result):\n",
    "    face_landmarks_list = detection_result.face_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected faces to visualize.\n",
    "    for idx in range(len(face_landmarks_list)):\n",
    "        face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "        # Draw the face landmarks.\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d3b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='facemesh_detector.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "\n",
    "    rgb = cv2.flip(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), 1)\n",
    "    mp_image = mp.Image(mp.ImageFormat.SRGB, rgb)\n",
    "\n",
    "    results = detector.detect(mp_image)\n",
    "\n",
    "    annotated_image = visualize(mp_image.numpy_view(), results)\n",
    "    cv2.imshow(\"Face Mesh\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47dd3056",
   "metadata": {},
   "source": [
    "## Pose estimation\n",
    "![](https://developers.google.com/static/mediapipe/images/solutions/examples/pose_detector.png)\n",
    "### References\n",
    "mediapipe introduction:\n",
    "- https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
    "- https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python\n",
    "\n",
    "mediapipe sample code:\n",
    "- https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O pose_detector.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8380dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "def visualize(rgb_image, detection_result):\n",
    "    pose_landmarks_list = detection_result.pose_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected poses to visualize.\n",
    "    for idx in range(len(pose_landmarks_list)):\n",
    "        pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "        # Draw the pose landmarks.\n",
    "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        pose_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            pose_landmarks_proto,\n",
    "            solutions.pose.POSE_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df83137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_detector.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "\n",
    "    rgb = cv2.flip(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB), 1)\n",
    "    mp_image = mp.Image(mp.ImageFormat.SRGB, rgb)\n",
    "\n",
    "    results = detector.detect(mp_image)\n",
    "\n",
    "    annotated_image = visualize(mp_image.numpy_view(), results)\n",
    "    cv2.imshow(\"Face Mesh\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0541847f",
   "metadata": {},
   "source": [
    "## object detection\n",
    "![](https://developers.google.com/static/mediapipe/images/solutions/examples/object_detector.png)\n",
    "### References\n",
    "mediapipe introduction:\n",
    "- https://developers.google.com/mediapipe/solutions/vision/object_detector/index\n",
    "- https://developers.google.com/mediapipe/solutions/vision/object_detector/python\n",
    "\n",
    "mediapipe sample code: \n",
    "- https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/object_detection/python/object_detector.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c68a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O object_detector.tflite -q https://storage.googleapis.com/mediapipe-tasks/object_detector/efficientdet_lite2_uint8.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a678f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import ImageFormat\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='object_detector.tflite')\n",
    "options = vision.ObjectDetectorOptions(base_options=base_options,\n",
    "                                       score_threshold=0.5)\n",
    "detector = vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, bgr = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb_mp = mp.Image(image_format=ImageFormat.SRGB, data=rgb)\n",
    "    \n",
    "    detection_result = detector.detect(rgb_mp)\n",
    "    \n",
    "    for detection in detection_result.detections:\n",
    "        # Draw bounding_box\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = bbox.origin_x, bbox.origin_y\n",
    "        end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "        cv2.rectangle(bgr, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "        # Draw label and score\n",
    "        category = detection.categories[0]\n",
    "        category_name = category.category_name\n",
    "        probability = round(category.score, 2)\n",
    "        result_text = category_name + ' (' + str(probability) + ')'\n",
    "        text_location = (MARGIN + bbox.origin_x,\n",
    "                         MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "        cv2.putText(bgr, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                    FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "    cv2.imshow('MediaPipe Object detection', bgr)\n",
    "    key = cv2.waitKey(5)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
